{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af92b71a-72ab-433c-9fca-6d2122efc471",
   "metadata": {},
   "source": [
    "# English-Swedish Generalizations & Clustering\n",
    "**Authors:** Riley Clark, Ishani Saha, Drew Marceau, Antoinette Reid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9aad2a9-6c4c-497a-874c-7aaafbf49bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Necessary Libraries\n",
    "import conllu\n",
    "import random\n",
    "random.seed(123)\n",
    "import math\n",
    "import gensim.downloader as gensim_api\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826a0d3-9f2f-479d-aa97-77121bdc26ad",
   "metadata": {},
   "source": [
    "# Task 1.2: English Syntax Generalization Practice\n",
    "We used the English GUM corpus with 233k entries maintained by Georgetown University Linguistics students. The text comes from multiple media formats. Our goal was to examine a generalization on verbs and their direct object pronouns within the corpus. The generalization accurately described the relevant sentences in the corpus.\n",
    "\n",
    "## Our Generalization\n",
    "**Expected Generalization:** Pronoun direct objects immediately follow their linked verbs.\n",
    "\n",
    "An example that illustrates this generalization is provided below.\n",
    "\n",
    "“First, our experimental subjects lived in a large enclosure under conditions that **allowed them** to exercise all day long.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fd1d24-4fd6-4368-b802-8eff3defc6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def englishGen(sentences, sample_size):\n",
    "    fit_generalization = []\n",
    "    possible_exceptions = []\n",
    "    current = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word[\"upos\"] == \"PRON\" and word[\"deprel\"] == \"obj\": # and word[\"lemma\"] != \"that\"\n",
    "                if word[\"head\"] != None and sentence[word[\"head\"]-1][\"upos\"] == \"VERB\":\n",
    "                    current[\"sentence\"] = sentence.metadata['text']\n",
    "                    current[\"PRON\"] = word\n",
    "                    current[\"VERB\"] = sentence[word[\"head\"]-1]\n",
    "                    if word[\"id\"] != 0 and word[\"id\"]-1 == word[\"head\"]:\n",
    "                        if current not in fit_generalization:\n",
    "                            fit_generalization.append(current.copy())\n",
    "                    elif word[\"id\"]-1 != word[\"head\"]: # != gets all possible exceptions\n",
    "                        if current not in possible_exceptions:\n",
    "                            possible_exceptions.append(current.copy())\n",
    "    \n",
    "    print(f\"Sentences that fit the generalization: {len(fit_generalization)}\\n\")\n",
    "    \n",
    "    fitted_samples = random.sample(fit_generalization, sample_size)\n",
    "    \n",
    "    for entry in fitted_samples:\n",
    "        print(f'PRON: {entry[\"PRON\"]}, VERB: {entry[\"VERB\"]}\\n Sentence: {entry[\"sentence\"]}\\n')\n",
    "    \n",
    "    print(f\"\\nSentences that may (or may not) be exceptions: {len(possible_exceptions)}\\n\")\n",
    "    \n",
    "    fitted_samples = random.sample(possible_exceptions, sample_size)\n",
    "    \n",
    "    for entry in fitted_samples:\n",
    "        print(f'PRON: {entry[\"PRON\"]}, VERB: {entry[\"VERB\"]}\\n Sentence: {entry[\"sentence\"]}\\n')\n",
    "    \n",
    "    #print(count)\n",
    "    print(f\"\\nTotal sentences with Pronouns linked to Verbs: {len(fit_generalization)+len(possible_exceptions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f40b56-2888-464b-b7e7-537f5a0d7895",
   "metadata": {},
   "source": [
    "## Results\n",
    "We used a function called 'englishGen' to search the corpus for sentences that contain pronoun (direct) objects linked to verbs. If those pronouns immediately followed the verb, then they fit our generalization. Otherwise, they were flagged as exceptions. Note that the deprel \"obj\" specifies only direct objects. If we wanted indirect objects we could use \"iobj\".\n",
    "\n",
    "About 86% of the corpus fit our generalization; direct object pronouns indeed follow their linked verbs. The exceptions to this are when adverbs modify the verb, adjectives modify the pronoun, wh-questions, a passivized verb requires argument movement to bring the direct object pronoun *forward* in the sentence, among others. Examples are provided below.\n",
    "\n",
    "## Examples of Exceptions\n",
    "### Wh-Question\n",
    "\"If your professor comes into an early morning class holding a mug of liquid, **what** do you assume she is **drinking**?\"\n",
    "\n",
    "### Stammer\n",
    "“You know, **documenting**, uh, uh, **whatever**.”\n",
    "\n",
    "### Modifying Preposition\n",
    "\"And I said ‘Ben, **pick** me out **something**, you've got fifty bucks to spend.'\"\n",
    "\n",
    "### Dialogue\n",
    "\"Whitmore told LA Weekly that on October 6 after traveling back from Florida, Montalvo ‘walked into lobby of the East L.A. station and turned himself in’, and **told** the police, ‘**everything** he did’.”\n",
    "\n",
    "### Wh-Movement\n",
    "“Malaysia and Indonesia have maintained a policy of turning away boats of migrants **which**, according to AFP, the United Nations and United States have both **criticised**.”\n",
    "\n",
    "### Movement\n",
    "\"**This** I would have **asked** him had he not been so far away, but he was very far, and could not be seen at all when he drew nigh that gigantic reef.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "208ec308-661c-4eb3-aa8f-8413eae22337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences that fit the generalization: 995\n",
      "\n",
      "PRON: it, VERB: like\n",
      " Sentence: And I really like it.\n",
      "\n",
      "PRON: her, VERB: distract\n",
      " Sentence: Back out in the mall, Cara is wailing, which could start an asthma attack, so to distract her I say, “You want a cookie?”\n",
      "\n",
      "PRON: em, VERB: get\n",
      " Sentence: You know then, they have to, like, keep em, away from anything, you know, get em really in the soft ground, and, no hard pebbles, or hard clods of dirt or anything?\n",
      "\n",
      "\n",
      "Sentences that may (or may not) be exceptions: 156\n",
      "\n",
      "PRON: which, VERB: live\n",
      " Sentence: Yet the turning - point is past, and history begins anew for us, the history which we shall live and act and others will write about.\n",
      "\n",
      "PRON: that, VERB: use\n",
      " Sentence: One of them is the channels that NBC as the broadcasting rights owner for the United States will use to air the Paralympic Games on.\n",
      "\n",
      "PRON: what, VERB: figure\n",
      " Sentence: No, I was just trying to figure out what we spent already.\n",
      "\n",
      "\n",
      "Total sentences with Pronouns linked to Verbs: 1151\n"
     ]
    }
   ],
   "source": [
    "with open(\"en_gum-ud-train.conllu\", encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "sentences_english = conllu.parse(data)\n",
    "englishGen(sentences_english, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe8f16-3a67-4a20-ba59-53715ccac564",
   "metadata": {},
   "source": [
    "# Tasks 1.1 & 1.3: Swedish Verb Negation Generalization\n",
    "We used the Swedish LinES corpus (from the Parallel Treebank of the same name) that includes just over 100k Swedish translations from English text. Our goal was to examine a generalization on Swedish verb negation. Our results conflicted with our expectations, and so we performed further examination to search for a different possible generalization.\n",
    "\n",
    "## Our Generalization\n",
    "**Expectated Generalization:** Negation words immediately follow the verb they negate.\n",
    "\n",
    "The below example illustrates this generalization. \n",
    "\n",
    "“Hon **svarade inte**.”\n",
    "(She didn't answer.)\n",
    "('inte' = 'not', linked words are bolded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461c20ea-b84d-4b3d-b155-c756891cd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swedishGen(sentences, sample_size):\n",
    "    fit_generalization = []\n",
    "    possible_exceptions = []\n",
    "    current = {}\n",
    "    exceptions = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word[\"xpos\"] == \"NEG\" and word[\"head\"] != None and sentence[word[\"head\"]-1][\"upos\"] == \"VERB\":\n",
    "            #if word[\"feats\"] != None and \"Polarity\" in word[\"feats\"].keys() and word[\"feats\"][\"Polarity\"] == \"Neg\" and and word[\"head\"] != None and sentence[word[\"head\"]-1][\"upos\"] == \"VERB\": # xpos \"NEG\", upos --\n",
    "                current[\"sentence\"] = sentence.metadata['text']\n",
    "                try:\n",
    "                    current[\"sentence-E\"] = sentence.metadata['text_en']\n",
    "                except:\n",
    "                    current[\"sentence-E\"] = None\n",
    "                current[\"NEG\"] = word\n",
    "                current[\"VERB\"] = sentence[word[\"head\"]-1]\n",
    "                if word[\"id\"] != 0 and word[\"id\"]-1 == word[\"head\"]:\n",
    "                    if current not in fit_generalization:\n",
    "                        fit_generalization.append(current.copy())\n",
    "                elif word[\"id\"]-1 != word[\"head\"]:\n",
    "                    if current not in possible_exceptions:\n",
    "                        possible_exceptions.append(current.copy())\n",
    "                        exceptions.append(sentence)\n",
    "                            \n",
    "    print(f\"Sentences that fit the generalization: {len(fit_generalization)}\\n\")\n",
    "    fitted_samples = random.sample(fit_generalization, min(sample_size, len(fit_generalization)))\n",
    "    for entry in fitted_samples:\n",
    "        print(f'NEG: {entry[\"NEG\"]}, VERB: {entry[\"VERB\"]}\\n'\n",
    "              f'Sentence: {entry[\"sentence\"]}\\n'\n",
    "              f'English Translation: {entry[\"sentence-E\"]}\\n')\n",
    "\n",
    "    print(f\"\\nSentences that may (or may not) be exceptions: {len(possible_exceptions)}\\n\")\n",
    "    fitted_samples = random.sample(possible_exceptions, min(sample_size, len(possible_exceptions)))\n",
    "    for entry in fitted_samples:\n",
    "        print(f'NEG: {entry[\"NEG\"]}, VERB: {entry[\"VERB\"]}\\n'\n",
    "              f'Sentence: {entry[\"sentence\"]}\\n'\n",
    "              f'English Translation: {entry[\"sentence-E\"]}\\n')\n",
    "\n",
    "    return exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec49e1d-7394-4f9e-a51e-01a94987cfc9",
   "metadata": {},
   "source": [
    "## Results & Discussion\n",
    "We used a function called 'swedishGen' to search the corpus for negation words linked to verbs. Then we filtered instances where the negation came immediately after the verb; those examples fit our generalization. The other sentences were cached as exceptions.\n",
    "\n",
    "Only about 21% of our corpus fit the expected generalization. There were a lot of exceptions ranging from embedded sentences, questions, auxiliaries, verb-object switches, and the list goes on. Some examples of these are provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27d93c1-9e49-43df-8d96-c0a23dd108ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences that fit the generalization: 99\n",
      "\n",
      "NEG: inte, VERB: visas\n",
      "Sentence: När du exporterar till Excel blir detaljfälten tillgängliga i pivottabellens verktygsfält, men fälten visas inte i rapporten.\n",
      "English Translation: When you export to Excel, detail fields will be available on the PivotTable toolbar in Excel, but the fields won't be displayed in the report.\n",
      "\n",
      "NEG: inte, VERB: insåg\n",
      "Sentence: Jag insåg inte genast vad det där skeppsbrottet verkligen innebar.\n",
      "English Translation: I did not see the real significance of that wreck at once.\n",
      "\n",
      "NEG: inte, VERB: tycker\n",
      "Sentence: \"Vi måste göda dig medan vi har chansen jag tycker inte den där skolmaten låter särskilt bra\"\n",
      "English Translation: We must feed you up while we've got the chance. I don't like the sound of that school food\n",
      "\n",
      "\n",
      "Sentences that may (or may not) be exceptions: 368\n",
      "\n",
      "NEG: inte, VERB: grälade\n",
      "Sentence: Mina föräldrar grälade och när de inte grälade snäste de åt varandra och när de inte snäste planerade de en fest, ordnade en fest, städade upp efter en fest.\n",
      "English Translation: My parents were rowing and when they weren't rowing they were snapping and when they weren't snapping they were planning a party, holding a party, cleaning up after a party.\n",
      "\n",
      "NEG: inte, VERB: godta\n",
      "Sentence: Men vi kan inte godta begreppet \"ekonomisk betydelse\" när det gäller rättighetsinnehavarna.\n",
      "English Translation: We can not, however, accept the concept of 'economic significance' for the rightholder.\n",
      "\n",
      "NEG: inte, VERB: avslutats\n",
      "Sentence: Vad jag har hört om representanten i Nicaragua tyder på att projektet inte har avslutats på ett riktigt sätt.\n",
      "English Translation: What I have been hearing about the representative in Nicaragua is indicative that the project had not been properly closed down.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"sv_lines-ud-train.conllu\", encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "sentences_swedish = conllu.parse(data)\n",
    "exceptions = swedishGen(sentences_swedish, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d44d49-924b-4fbc-89c4-4d0ce3ceadbf",
   "metadata": {},
   "source": [
    "## Secondary Corpus Test\n",
    "When tested with a slightly smaller corpus (96k entries) called Talbanken from Lund University. The sentences were taken from various text genres like textbooks, brochures, and newspaper articles. We found similar results to the above, where 22% of the sentences with negated verbs actually fit our generalization. This leads us to believe that the translation bias in our first corpus may not be the reason that our generalization fits so poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951918d9-ef2e-4dd5-8ba2-1848f7ccf61a",
   "metadata": {},
   "source": [
    "# Generalization Exceptions\n",
    "Our generalization seems to hold on simple sentences with little to no nuance.\n",
    "\n",
    " Examples include:\n",
    "\n",
    "“Hon svarade inte.” -“She didn't answer.”\n",
    "\n",
    "\"Hon talar inte jiddisch?\" - “She doesn't speak Yiddish?”\n",
    "\n",
    "These sentences relay straightforward information and do not contain many flourishes in speech. If we were to only consider such sentencs, our generalization holds with 21% accuracy on verb negations.\n",
    "\n",
    "However, if we take into account more detailed sentences, we see a diffrerent result. In examining our initial results, there were two main exceptions we identified that change the location of negation. If we include this nuance, accuracy increases to 44%\n",
    "\n",
    "- **Auxiliary verbs:** if auxilary verbs are present, the negation is placed between the auxiliary and the main (head) verb\n",
    "\n",
    "“Hans sekreterare hade inte ringt det samtal hon hade fått instruktioner om.”\n",
    "\n",
    "“His secretary had not made the instructed call.”\n",
    "\t\n",
    "- **Embedded clauses:** if there is an embedded clause, the negation follows the subject of the clause and preceedes the main (head) verb. This seems to happen because of rules regarding VPs in Swedish\n",
    "\n",
    "“Jag har suttit här tålmodigt och jag finner det anmärkningsvärt att ni inte ropar upp mig.”\n",
    "\n",
    "“I have sat here patiently and I find it quite extraordinary that you are not calling me.”\n",
    "\n",
    "While 44% accuracy might seem low, this result can be explained by recognizing a feature of Swedish that lets words be reordered to put emphasis on certain aspects of the sentence. For example, Object Shift allows for the object of a verb to swap places with the negation, while still producing a gramatical sentence\n",
    "\n",
    "“Jag förstår det inte alls.”\n",
    "\n",
    "“I do not understand it at all.”\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82b8bd5b-2593-43e1-ab35-5a05746bb53b",
   "metadata": {},
   "source": [
    "# Substitute into swedishGen for Aux checking\n",
    "            if word[\"upos\"] == \"AUX\" and word[\"head\"] is not None:\n",
    "                head_idx = word[\"head\"] - 1\n",
    "                if sentence[head_idx][\"upos\"] == \"VERB\":\n",
    "\n",
    "                    if word[\"id\"] < len(sentence) and sentence[word[\"id\"]][\"xpos\"] == \"NEG\":\n",
    "                        current = {}\n",
    "                        current[\"sentence\"] = sentence.metadata.get(\"text\")\n",
    "                        try:\n",
    "                            current[\"sentence-E\"] = sentence.metadata[\"text_en\"]\n",
    "                        except KeyError:\n",
    "                            current[\"sentence-E\"] = sentence.metadata.get(\"Text_en\", None)\n",
    "\n",
    "                        current[\"AUX\"] = word\n",
    "                        current[\"NEG\"] = sentence[word[\"id\"]] \n",
    "                        current[\"VERB\"] = sentence[head_idx]\n",
    "\n",
    "                        if word[\"id\"] - 1 <= word[\"head\"]:\n",
    "                            if current not in fit_generalization:\n",
    "                                fit_generalization.append(current.copy())\n",
    "                        elif word[\"id\"] - 1 != word[\"head\"]:\n",
    "                            if current not in possible_exceptions:\n",
    "                                possible_exceptions.append(current.copy())\n",
    "                                exceptions.append(sentence)\n",
    "            if word[\"xpos\"] == \"NEG\":\n",
    "                if word[\"head\"] != None and sentence[word[\"head\"]-1][\"upos\"] == \"VERB\":\n",
    "                    current[\"sentence\"] = sentence.metadata['text']\n",
    "                    try:\n",
    "                        current[\"sentence-E\"] = sentence.metadata['text_en']\n",
    "                    except:\n",
    "                        try:\n",
    "                            current[\"sentence-E\"] = sentence.metadata['Text_en']\n",
    "                        except:\n",
    "                            print(f\"Error for sentence: {sentence.metadata}\")\n",
    "                            current[\"sentence-E\"] = None\n",
    "                    current[\"NEG\"] = word\n",
    "                    current[\"VERB\"] = sentence[word[\"head\"]-1]\n",
    "                    current[\"AUX\"] = None\n",
    "                    if word[\"id\"] != 0 and word[\"id\"]-1 >= word[\"head\"]:\n",
    "                        if current not in fit_generalization:\n",
    "                            fit_generalization.append(current.copy())\n",
    "                    elif word[\"id\"]-1 != word[\"head\"]:\n",
    "                        if current not in possible_exceptions:\n",
    "                            possible_exceptions.append(current.copy())\n",
    "                            exceptions.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b0c37-75ae-4370-bd76-ac58759009a8",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "## Verb Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cae1e7-7f7d-4634-92cf-d5d61cc42419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_frequencies(sentences):\n",
    "    verb_freq = {}\n",
    "    # verbs = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word[\"upos\"] == \"VERB\":\n",
    "                verb = word[\"lemma\"]\n",
    "                verb_freq[verb] = verb_freq.get(verb,0) + 1\n",
    "    verbs = list(verb_freq.keys())\n",
    "    print(f\"There are {len(verbs)} verbs used in the corpus.\")\n",
    "    #print(f\"Those verbs are: {verbs}\")\n",
    "    sorted_by_frequency_desc = sorted(verb_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "    first_five = sorted_by_frequency_desc[:5] # optionally random sample the top 20%\n",
    "    # it would be better if we can print this in a nicer format, and include the english translation\n",
    "    print(f\"The highest frequency verbs are: {first_five}\")\n",
    "    # then find middle five using some multiplication for the sorted list?\\\n",
    "    middle_five = sorted_by_frequency_desc[len(sorted_by_frequency_desc)//5:len(sorted_by_frequency_desc)//5+5]\n",
    "    print(f\"Some of the mid-frequency verbs are: {middle_five}\")\n",
    "\n",
    "    #choosing a randomized set from top 20%\n",
    "    arbitrary_top20 = [sorted_by_frequency_desc[x] for x in random.sample(range(len(verbs)//5), 5)]\n",
    "    arbitrary_between20and40 = [sorted_by_frequency_desc[x] for x in random.sample(range(len(verbs)//5,2*len(verbs)//5), 5)]\n",
    "    return arbitrary_top20 + arbitrary_between20and40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96354d-2062-4ad6-a4bf-47a14be64d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1308 verbs used in the corpus.\n",
      "The highest frequency verbs are: [('säga', 294), ('ha', 225), ('komma', 188), ('se', 185), ('gå', 180)]\n",
      "Some of the mid-frequency verbs are: [('misstänka', 5), ('erinra', 5), ('stoppa', 5), ('lukta', 5), ('vila', 5)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lägga', 47),\n",
       " ('räcka', 17),\n",
       " ('hänga', 19),\n",
       " ('hälsa', 8),\n",
       " ('tränga', 8),\n",
       " ('dränka', 3),\n",
       " ('besvara', 4),\n",
       " ('misstänka', 5),\n",
       " ('testa', 2),\n",
       " ('uppmana', 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_frequencies(sentences_swedish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f39d0-5ef5-49f6-ba62-af63cc8e7ad3",
   "metadata": {},
   "source": [
    "The verbs chosen are randomly sampled from the top 20% of verbs when sorted by frequency and the next 20% of most frequently used verbs -- five from each category. Frequency was determined by tallying the lemmas of each verb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3e737-9114-4133-bb63-198a95f09306",
   "metadata": {},
   "source": [
    "## Verb Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2659514-de87-4ef7-856c-67abbf5049c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sets(sentences, verb):\n",
    "    sets = {\"verb\": verb, \"subjects\": set(), \"objects\": set(), \"modifiers\": set(), \"before\": set(), \"after\": set(), \"clausalcomps\": set()}\n",
    "    for sentence in sentences:\n",
    "        words = [x['lemma'] for x in sentence]\n",
    "        if (verb in words):\n",
    "            word_id = words.index(verb)+1\n",
    "            sets[\"before\"].add(words[word_id-2])\n",
    "            sets[\"after\"].add(words[word_id])\n",
    "            for word in sentence:\n",
    "                if(word[\"deprel\"] in [\"obj\", \"nsubj\", \"iobj\", \"advmod\", \"ccomp\"] and word[\"head\"] == word_id):\n",
    "                    match word[\"deprel\"]:\n",
    "                        case \"obj\" | \"iobj\":\n",
    "                            if word[\"upos\"]!= \"PROPN\": sets[\"objects\"].add(word[\"lemma\"])\n",
    "                           # sets[\"objects\"].add(word[\"lemma\"])\n",
    "                        case \"nsubj\":\n",
    "                            if word[\"upos\"]!= \"PROPN\": sets[\"subjects\"].add(word[\"lemma\"]) \n",
    "                            #sets[\"subjects\"].add(word[\"lemma\"])\n",
    "                        case \"advmod\": #includes negation\n",
    "                            sets[\"modifiers\"].add(word[\"lemma\"])\n",
    "                        case \"ccomp\":\n",
    "                            sets[\"clausalcomps\"].add(word[\"lemma\"])\n",
    "                    \n",
    "    return sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e590f-584b-4192-9140-87fda42894ae",
   "metadata": {},
   "source": [
    "For any verb's *lemma* in the set of Swedish sentences, this method generates a dictionary containing sets of each subjects, objects, modifiers, preceding words, and following words corresponding to the given verb. The set of modifiers for the verb only contains adverbs, (or those with the dependency relationship \"advmod\" to the verb), but not other modifiers like negation, prepositions, or auxiliaries. Including would have likely skewed our results by adding more noise, since semantically, adverbs might be more significant. However, it may have been beneficial to have include other modifiers as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30044b49-804b-4398-97cf-74a04a285a50",
   "metadata": {},
   "source": [
    "## Word 2 Vector Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f8ca3-2d3c-4c89-9288-127cf830bf49",
   "metadata": {},
   "source": [
    "After we have the sets of words that we need, we need to make the Word2Vec model. This is what the following function does, returning it in the form of a space so that we can use gensim library functions on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cb9878a-c409-4b1e-8c51-061d92f294a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word2vec model\n",
    "def make_W2V(conllu_corpus):\n",
    "    sentences = []\n",
    "    for tokList in conllu_corpus:\n",
    "        sent = []\n",
    "        for token in tokList:\n",
    "            if token != \"metadata\":\n",
    "                sent.append(token[\"lemma\"])\n",
    "        sentences.append(sent)\n",
    "        \n",
    "    space = Word2Vec(sentences, epochs=10, min_count=0, vector_size=300, sg = 1)\n",
    "    return space.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee89fb-91f8-448f-9bc4-5814f647ddfc",
   "metadata": {},
   "source": [
    "Now that we have the Word2Vec, we can compute the k nearest words semantically from the Word2Vec vectors. We can also find the centroid of each set by summing all of the word vectors in the set and finding the most similar vector to the sum. Note that the similarity is done via cosine similarity, so we do not need to divide the sum by the number of words (which would give the average vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5414b46f-5f51-4185-a3fe-2c50aa0b6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest(k, space, vector):\n",
    "    if vector == \"empty set\":\n",
    "        return \"empty set\"\n",
    "    return space.most_similar(vector)[:k]\n",
    "\n",
    "def find_centroid(set: set, space):\n",
    "    total = []\n",
    "    if len(set) == 0:\n",
    "        return [\"empty set\"]\n",
    "    for token in set:\n",
    "        total.append(space[token])\n",
    "    \n",
    "    sum = reduce(lambda x, y: x + y, total)\n",
    "    return space.similar_by_vector(sum)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "892d19fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 words most similar to 'ha' ('have') in the subjects set are: ['railway', 'literature', 'ty', 'roll', 'presuppose']\n",
      "The top 5 words most similar to 'ha' ('have') in the objects set are: ['conviction', 'Clelia', 'greatness', 'Society', 'rinse']\n",
      "The top 5 words most similar to 'ha' ('have') in the modifiers set are: ['lust', 'before', 'bad', 'away', 'suspect']\n",
      "The top 5 words most similar to 'ha' ('have') in the before set are: ['wonder', 'pay', 'intention', 'marriage', 'Jerusalem']\n",
      "The top 5 words most similar to 'ha' ('have') in the after set are: ['Wonderful', 'home', 'immediately', 'Margot', 'everything']\n"
     ]
    }
   ],
   "source": [
    "def outputTopK(label, verb, sets, space, svToEn, translate=True, k=5):\n",
    "    topK = k_nearest(k, space, find_centroid(sets[label], space)[0])\n",
    "    arr = []\n",
    "    if topK == \"empty set\":\n",
    "        print(f'The {label} set is empty for verb {verb}')\n",
    "        return\n",
    "    for pair in topK:\n",
    "        if translate: arr.append(svToEn[pair[0]])\n",
    "        else: arr.append(pair[0])\n",
    "    print(f'The top {k} words most similar to \\'{verb}\\' (\\'{svToEn[verb]}\\') in the {label} set are: {arr}')\n",
    "\n",
    "def outputAllSets(verb, k=5):\n",
    "    sets = gen_sets(sentences_swedish, verb)\n",
    "    space = make_W2V(sentences_swedish)\n",
    "    with open('dictionary.json', 'r') as f:\n",
    "        svToEn = json.load(f)\n",
    "    outputTopK(\"subjects\", verb, sets, space, svToEn)\n",
    "    outputTopK(\"objects\", verb, sets, space, svToEn)\n",
    "    outputTopK(\"modifiers\", verb, sets, space, svToEn)\n",
    "    outputTopK(\"before\", verb, sets, space, svToEn)\n",
    "    outputTopK(\"after\", verb, sets, space, svToEn)\n",
    "\n",
    "\n",
    "outputAllSets(\"ha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b2d98",
   "metadata": {},
   "source": [
    "## A. Which out of the subject/object/modifier/before-after sets gives the best sense of the meaning of the verb?\n",
    "'ha' means 'to have' or 'to possess'. It seems that the object set gives the best sense of the meaning of the verb, with phrases like 'to have hope', 'to possess horse', 'to have hold', and 'to possess greatness' (these exact sentences may not be shown because the Word2Vec model is nondeterministic). Keep in mind, these are the lemma versions of the words, so they may mean different things when detached from the context. The after set also seems to line up well with the meaning of the verb, however it would be more difficult to understand it without being given the verb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde5152",
   "metadata": {},
   "source": [
    "## B. Does frequency of the verb have an effect on the kinds of clusters you end up with?\n",
    "Yes and no. The function we use for vector similarity implements cosine similarity which does not depend on the length of the vector, so in that sense the frequency does not matter. However, it is likely that if a verb appears more often in the corpus, then its meaning will become more specific to what it actually means because there is a higher chance to see all uses of it. This would give the verb a better embedding in the vector space, which may then affect the clusters that appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e31e4ce",
   "metadata": {},
   "source": [
    "# C. One additional experiment: Clausal Component Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0630c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 8 words most similar to 'ha' ('have') in the clausalcomps set are: ['fly', 'send', 'go', 'think', 'go', 'stop', 'set', 'jab']\n",
      "The top 8 words most similar to 'komma' ('come') in the clausalcomps set are: ['exist', 'express', 'miss', 'opportunity', 'opinion', 'event', 'roll', 'beginning']\n",
      "The top 8 words most similar to 'se' ('see') in the clausalcomps set are: ['clock', 'parent', 'set', 'Boy', 'fetch', 'quiet', 'then', 'laugh']\n",
      "The top 8 words most similar to 'tala' ('say') in the clausalcomps set are: ['meet', 'of course', 'right', 'imagine', 'hope', 'already', 'forget', 'yet']\n"
     ]
    }
   ],
   "source": [
    "space = make_W2V(sentences_swedish)\n",
    "with open('dictionary.json', 'r') as f:\n",
    "    svToEn = json.load(f)\n",
    "\n",
    "verbs = [\"ha\", \"komma\", \"se\", \"tala\"]\n",
    "\n",
    "for verb in verbs:\n",
    "    sets = gen_sets(sentences_swedish, verb)\n",
    "    outputTopK(\"clausalcomps\", verb, sets, space, svToEn, k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c45d8",
   "metadata": {},
   "source": [
    "What we can infer from these lists:  \n",
    "&emsp;1. 'ha' seems to take a variety of verbs, suggesting that it is a very general verb in Swedish. This tracks with it being among the most frequent words in the corpus. More specifically, though, we can say that it is likely to take stative predicates.  \n",
    "\n",
    "&emsp;2. 'komma' takes mostly nouns/adjectives, which suggests that rather than the action meaning of 'come', 'komma' is used more in evaluative predications ('came to be', 'came to know'). Although, it is strange that there are little to no verbs, so perhaps the data is just noisy.  \n",
    "\n",
    "&emsp;3. 'se' seems to be a perception verb, taking words like 'leave', 'each other', 'fetch', etc. It is used to set up concrete actions.  \n",
    "\n",
    "&emsp;4. 'tala' is the cleanest cluster, relating most to cognition verbs ('hope', 'wonder', 'mean'). This shows us that it is likely to mean something about describing thought or attitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c35c0d",
   "metadata": {},
   "source": [
    "To conclude, the clausal complement set is able to tell us a little bit about the meaning of the given verb, more so for some than others. For example, 'komma' is used in a much more mixed context than 'tala', so it is a bit easier to tell what 'tala' means than to tell what 'komma' means. Overall though, it is not very obvious what verb was used just by looking at the sets. It helps to narrow down the search to broad meanings like perception verbs, general-use verbs, cognition verbs, etc., but not to the specific meaning of the verb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
