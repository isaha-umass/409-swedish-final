{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569de287-e7cd-4c06-9c75-859442428920",
   "metadata": {},
   "source": [
    "# Working with Universal Dependencies\n",
    "\n",
    "The Universal Dependencies framework distributes annotated corpora for many languages, all using the same dependency format. In this notebook, I'll demonstrate how to access such a corpus from within Python. \n",
    "\n",
    "We will work with English GUM corpus. You can find it listed under \"English\" on https://universaldependencies.org/#language- The repository with the actual corpus is at https://github.com/UniversalDependencies/UD_English-GUM/tree/master\n",
    "\n",
    "From the github repository, please download the training portion of the corpus, and put it in the same directory as this notebook. \n",
    "\n",
    "First, we will take a look at the format in which Universal Dependencies corpora are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bcf2f5-0b05-4a70-9a96-06346284ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have conllu yet, uncomment the following\n",
    "# !python -m pip install --upgrade conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae05adf-0fe1-4d76-ba45-100c4ae55bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu # reading Universal Dependency files in the CONLLu format\n",
    "import math\n",
    "import gensim.downloader as gensim_api\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import stats\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528f98d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\sokkm\\\\Documents\\\\SCHOOL\\\\2025-2026\\\\ling409\\\\409-swedish-final\\\\.venv\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75cf88-7d78-4868-9b71-05a30f5bb139",
   "metadata": {},
   "source": [
    "We open the GUM corpus as a text file, and look at its first few lines. After the initial metadata, the first sentence starts with the line\n",
    "      \"# text = Aesthetic Appreciation and Spanish Art:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e103158-0fd5-4666-9713-ade344abd881",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"en_gum-ud-train.conllu\", encoding=\"utf8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1946b07f-aa17-4b31-81db-c928176b4fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newdoc id = GUM_academic_art\n",
      "# global.Entity = GRP-etype-infstat-salience-centering-minspan-link-identity\n",
      "# meta::author = Claire Bailey-Ross, Andrew Beresford, Daniel Smith, Claire Warwick\n",
      "# meta::dateCollected = 2017-09-13\n",
      "# meta::dateCreated = 2017-08-08\n",
      "# meta::dateModified = 2017-09-13\n",
      "# meta::genre = academic\n",
      "# meta::salientEntities = 4 (5*), 5 (5*), 44 (5*), 45 (5*), 46 (5*), 47 (5*), 27 (4*), 147 (4*), 2 (3*), 43 (3), 20 (2*), 23 (2), 63 (2), 72 (2), 73 (2), 3 (1), 19 (1), 24 (1), 26 (1), 48 (1), 49 (1), 50 (1), 62 (1), 68 (1), 69 (1), 74 (1), 76 (1), 77 (1), 78 (1), 79 (1), 158 (1)\n",
      "# meta::sourceURL = https://dh2017.adho.org/abstracts/333/333.pdf\n",
      "# meta::speakerCount = 0\n",
      "# meta::summary1 = (human) This paper presents an eye tracking study to explore how viewers experience art, focusing on a 17th Century collection of Spanish paintings by Zurbarán.\n",
      "# meta::summary2 = (claude-3-5-sonnet-20241022) This pilot study uses eye-tracking techniques to examine how viewers visually process and aesthetically experience a unique collection of 17th Century Zurbarán paintings at Auckland Castle, investigating the effects of written interpretation on art viewing behavior.\n",
      "# meta::summary3 = (gpt4o; postedited) Using eye-tracking, this study examines audience engagement with 17th Century Zurbarán paintings, providing insights into aesthetic appreciation and visual processing, exploring the influence of museum labels and implications for gallery practices, integrating Spanish art history, psychology, digital humanities, and museum studies.\n",
      "# meta::summary4 = (Llama-3.2-3B-Instruct) Researchers used eye-tracking techniques to study how people visually explore and experience 17th-century Spanish art, including the Jacob cycle by Zurbarán, and how written labels affect their behavior.\n",
      "# meta::summary5 = (Qwen2.5-7B-Instruct) A collaborative pilot project uses eye-tracking techniques to analyze how visitors visually explore and aesthetically react to a unique collection of 17th century Zurbarán paintings in order to gain insights into their viewing behaviors and potentially improve museum practices.\n",
      "# meta::title = Aesthetic Appreciation and Spanish Art: Insights from Eye-Tracking\n",
      "# newpar\n",
      "# newpar_block = head (2 s) | hi rend:::\"bold blue\" (2 s)\n",
      "# sent_id = GUM_academic_art-1\n",
      "# s_type = frag\n",
      "# s_prominence = 2\n",
      "# transition = establishment\n",
      "# text = Aesthetic Appreciation and Spanish Art:\n",
      "1\tAesthetic\taesthetic\tADJ\tJJ\tDegree=Pos\t2\tamod\t2:amod\tDiscourse=organization-heading:1->57:8:grf-ly-_-_+sem-lxchn-1,671-_+sem-lxchn-4,619-_+sem-lxchn-5,620-_|Entity=(1-abstract-new-nnnnn-cf1-2-sgl|MSeg=Aesthet-ic\n",
      "2\tAppreciation\tappreciation\tNOUN\tNN\tNumber=Sing\t0\troot\t0:root\tEntity=1)|MSeg=Appreciat-ion\n",
      "3\tand\tand\tCCONJ\tCC\t_\t5\tcc\t5:cc\t_\n",
      "4\tSpanish\tSpanish\tADJ\tJJ\tDegree=Pos\t5\tamod\t5:amod\tEntity=(2-abstract-new-snssn-cf2-2-coref|MSeg=Span-ish\n",
      "5\tArt\tart\tNOUN\tNN\tNumber=Sing\t2\tconj\t2:conj:and\tEntity=2)|SpaceAfter=No\n",
      "6\t:\t:\tPUNCT\t:\t_\t2\tpunct\t2:punct\t_\n",
      "\n",
      "# sent_id = GUM_academic_art-2\n",
      "# s_type = frag\n",
      "# s_prominence = 3\n",
      "# transition = null\n",
      "# text = Insights from Eye-Tracking\n",
      "1\tInsights\tinsight\tNOUN\tNNS\tNumber=Plur\t0\troot\t0:root\tDiscourse=elaboration-additional:2->1:0:grf-col-6-gold|Entity=(3-abstract-new-nnnns-cf1-1-coref|MSeg=In-sigh-t-s\n",
      "2\tfrom\tfrom\tADP\tIN\t_\t5\tcase\t5:case\t_\n",
      "3\tEye\teye\tNOUN\tNN\tNumber=Sing\t5\tcompound\t5:compound\tEntity=(4-abstract-new-sssss-cf2-3-coref(5-object-new-sssss-cf3-1-coref)|SpaceAfter=No|XML=<w>\n",
      "4\t-\t-\tPUNCT\tHYPH\t_\t3\tpunct\t3:punct\tSpaceAfter=No\n",
      "5\tTracking\ttracking\tNOUN\tNN\tNumber=Sing\t1\tnmod\t1:nmod:from\tEntity=4)3)|MSeg=Track-ing|XML=</w>\n",
      "\n",
      "# newpar\n",
      "# newpar_block = p (1 s)\n",
      "# sent_id = GUM_academic_art-3\n",
      "# s_type = frag\n",
      "# s_prominence = 2\n",
      "# transition = null\n",
      "# text = Claire Bailey-Ross claire.bailey-ross@port.ac.uk University of Portsmouth, United Kingdom\n",
      "1\tClaire\tClaire\tPROPN\tNNP\tNumber=Sing\t0\troot\t0:root\tDiscourse=attribution-positive:3->57:7:sem-atsrc-12-46-_+syn-rpr-676-_|Entity=(6-person-new-nnnnn-cf1-1,2,4-coref\n",
      "2\tBailey\tBailey\tPROPN\tNNP\tNu\n"
     ]
    }
   ],
   "source": [
    "print(data[:4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc952160-39df-4b1f-9527-54240f9da389",
   "metadata": {},
   "source": [
    "As you can see, this is a tabular format. There is a line for each word in the sentence, and the information for that word is given in tab-delimited cells, for example:\n",
    "\n",
    "```2\tAppreciation\tappreciation\tNOUN\tNN\tNumber=Sing\t0\troot\t0:root\tEntity=1)|MSeg=Appreciat-ion```\n",
    "\n",
    "This is the CoNLL-U format, which originated with a shared task at the Conference on Natural Language Learning (CoNLL). \n",
    "\n",
    "There is a Python package, conllu, that is made for reading CoNLL-U data. Once we have read the GUM corpus into a string, we can parse it with conllu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef8f2e94-c22a-4fca-9442-ff4620567801",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = conllu.parse(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45e066-23f5-4fdf-827e-b65ff1a5adaf",
   "metadata": {},
   "source": [
    "The content of `sentences` is a sequence of TokenList objects. Here is the one for the 10th sentence of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d2333cc-4aba-4d54-ae98-76a3fc1b57d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<Thus, ,, the, time, it, takes, and, the, ways, of, visually, exploring, an, artwork, can, inform, about, its, relevance, ,, interestingness, ,, and, even, its, aesthetic, appeal, ., metadata={sent_id: \"GUM_academic_art-11\", s_type: \"sub\", s_prominence: \"3\", transition: \"establishment\", text: \"Thus, the time it takes and the ways of visually exploring an artwork can inform about its relevance, interestingness, and even its aesthetic appeal.\"}>\n"
     ]
    }
   ],
   "source": [
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836027d-685d-4613-b4b1-89ae92254c4a",
   "metadata": {},
   "source": [
    "We can access the entries on the TokenList through a for-loop, or using an index. Here is the first token of sentence 10. As you can see, it is a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba93605-0b32-4f1f-b35f-34d2421814cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'form': 'Thus',\n",
       " 'lemma': 'thus',\n",
       " 'upos': 'ADV',\n",
       " 'xpos': 'RB',\n",
       " 'feats': None,\n",
       " 'head': 16,\n",
       " 'deprel': 'advmod',\n",
       " 'deps': [('advmod', 16)],\n",
       " 'misc': {'Discourse': 'context-background:12->23:7:_',\n",
       "  'PDTB': 'Explicit:Contingency.Cause.Result:thus:107:79-106:108-134',\n",
       "  'SpaceAfter': 'No'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence10 = sentences[10]\n",
    "firstword = sentence10[0]\n",
    "firstword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f591f3d7-7f12-47eb-b7e0-ae2dadf1b4cc",
   "metadata": {},
   "source": [
    "You can access the entries in that dictionary by their keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52dd8ff8-ca4e-4f16-9231-865453e85431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thus'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstword[\"lemma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2bfd8-91dd-4a60-a737-ca819e741975",
   "metadata": {},
   "source": [
    "To better understand this big dictionary, it helps to view it as an attribute-value matrix. Here is the first word of the 10th sentence of the UD_English-GUM corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fba85e-117a-40bc-b2ba-1f09995b47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstword = {'id': 1,\n",
    "  'form': 'Thus',\n",
    "  'lemma': 'thus',\n",
    "  'upos': 'ADV',\n",
    "  'xpos': 'RB',\n",
    "  'feats': None,\n",
    "  'head': 16,\n",
    "  'deprel': 'advmod',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98da69-4ce7-4805-8bb1-f69609ba10a7",
   "metadata": {},
   "source": [
    "This is the following attribute-value matrix (AVM):\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "\\text{id:} & 1\\\\\n",
    "\\text{form:} & 'Thus'\\\\\n",
    "\\text{lemma:} & 'thus'\\\\\n",
    "\\text{upos:} &  'ADV'\\\\\n",
    "\\text{xpos:} & 'RB'\\\\\n",
    "\\text{feats:} &  None\\\\\n",
    "\\text{head:} & 16\\\\\n",
    "\\text{deprel:}  & advmod\\\\\n",
    "\\text{deps:}  & None\\\\\n",
    "\\text{misc:} & \\left[\\begin{array}{ll}\n",
    "\\text{SpaceAfter:} & 'No'\n",
    "\\end{array}\\right]\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "As you saw above, you can access an entry in this attribute-value matrix through its dictionary key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e51ee5-055c-4d1c-8176-751bab20de51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thus'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstword[\"lemma\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f84425-20f4-4249-a478-5c258ffc9a6a",
   "metadata": {},
   "source": [
    "One of the values in the AVM is itself an AVM. To access the value that tells you whether there is a space after the word, you need to specify the whole path of keys. `firstword[\"misc\"]` accesses a dictionary, namely `{'SpaceAfter': 'No'}`, which again has keys, in particular `SpaceAfter`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4578946-d0d3-49ef-90e0-50f0f092919f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstword[\"misc\"][\"SpaceAfter\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0425615d-45b3-45fb-aa3f-48af620c03d1",
   "metadata": {},
   "source": [
    "The Universal Dependencies representation of a whole sentence is a list of tokens, that is, a list of dictionaries (=AVMs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de31cd0-1665-4d18-b683-d3448fe71b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence10 = [{'id': 1,\n",
    "  'form': 'Thus',\n",
    "  'lemma': 'thus',\n",
    "  'upos': 'ADV',\n",
    "  'xpos': 'RB',\n",
    "  'feats': None,\n",
    "  'head': 16,\n",
    "  'deprel': 'advmod',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}},\n",
    " {'id': 2,\n",
    "  'form': ',',\n",
    "  'lemma': ',',\n",
    "  'upos': 'PUNCT',\n",
    "  'xpos': ',',\n",
    "  'feats': None,\n",
    "  'head': 1,\n",
    "  'deprel': 'punct',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 3,\n",
    "  'form': 'the',\n",
    "  'lemma': 'the',\n",
    "  'upos': 'DET',\n",
    "  'xpos': 'DT',\n",
    "  'feats': {'Definite': 'Def', 'PronType': 'Art'},\n",
    "  'head': 4,\n",
    "  'deprel': 'det',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 4,\n",
    "  'form': 'time',\n",
    "  'lemma': 'time',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 16,\n",
    "  'deprel': 'nsubj',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 5,\n",
    "  'form': 'it',\n",
    "  'lemma': 'it',\n",
    "  'upos': 'PRON',\n",
    "  'xpos': 'PRP',\n",
    "  'feats': {'Case': 'Nom',\n",
    "   'Gender': 'Neut',\n",
    "   'Number': 'Sing',\n",
    "   'Person': '3',\n",
    "   'PronType': 'Prs'},\n",
    "  'head': 6,\n",
    "  'deprel': 'nsubj',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 6,\n",
    "  'form': 'takes',\n",
    "  'lemma': 'take',\n",
    "  'upos': 'VERB',\n",
    "  'xpos': 'VBZ',\n",
    "  'feats': {'Mood': 'Ind',\n",
    "   'Number': 'Sing',\n",
    "   'Person': '3',\n",
    "   'Tense': 'Pres',\n",
    "   'VerbForm': 'Fin'},\n",
    "  'head': 4,\n",
    "  'deprel': 'acl:relcl',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 7,\n",
    "  'form': 'and',\n",
    "  'lemma': 'and',\n",
    "  'upos': 'CCONJ',\n",
    "  'xpos': 'CC',\n",
    "  'feats': None,\n",
    "  'head': 9,\n",
    "  'deprel': 'cc',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 8,\n",
    "  'form': 'the',\n",
    "  'lemma': 'the',\n",
    "  'upos': 'DET',\n",
    "  'xpos': 'DT',\n",
    "  'feats': {'Definite': 'Def', 'PronType': 'Art'},\n",
    "  'head': 9,\n",
    "  'deprel': 'det',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 9,\n",
    "  'form': 'ways',\n",
    "  'lemma': 'way',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NNS',\n",
    "  'feats': {'Number': 'Plur'},\n",
    "  'head': 4,\n",
    "  'deprel': 'conj',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 10,\n",
    "  'form': 'of',\n",
    "  'lemma': 'of',\n",
    "  'upos': 'SCONJ',\n",
    "  'xpos': 'IN',\n",
    "  'feats': None,\n",
    "  'head': 12,\n",
    "  'deprel': 'mark',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 11,\n",
    "  'form': 'visually',\n",
    "  'lemma': 'visually',\n",
    "  'upos': 'ADV',\n",
    "  'xpos': 'RB',\n",
    "  'feats': None,\n",
    "  'head': 12,\n",
    "  'deprel': 'advmod',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 12,\n",
    "  'form': 'exploring',\n",
    "  'lemma': 'explore',\n",
    "  'upos': 'VERB',\n",
    "  'xpos': 'VBG',\n",
    "  'feats': {'VerbForm': 'Ger'},\n",
    "  'head': 9,\n",
    "  'deprel': 'acl',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 13,\n",
    "  'form': 'an',\n",
    "  'lemma': 'a',\n",
    "  'upos': 'DET',\n",
    "  'xpos': 'DT',\n",
    "  'feats': {'Definite': 'Ind', 'PronType': 'Art'},\n",
    "  'head': 14,\n",
    "  'deprel': 'det',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 14,\n",
    "  'form': 'artwork',\n",
    "  'lemma': 'artwork',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 12,\n",
    "  'deprel': 'obj',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 15,\n",
    "  'form': 'can',\n",
    "  'lemma': 'can',\n",
    "  'upos': 'AUX',\n",
    "  'xpos': 'MD',\n",
    "  'feats': {'VerbForm': 'Fin'},\n",
    "  'head': 16,\n",
    "  'deprel': 'aux',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 16,\n",
    "  'form': 'inform',\n",
    "  'lemma': 'inform',\n",
    "  'upos': 'VERB',\n",
    "  'xpos': 'VB',\n",
    "  'feats': {'VerbForm': 'Inf'},\n",
    "  'head': 0,\n",
    "  'deprel': 'root',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 17,\n",
    "  'form': 'about',\n",
    "  'lemma': 'about',\n",
    "  'upos': 'ADP',\n",
    "  'xpos': 'IN',\n",
    "  'feats': None,\n",
    "  'head': 19,\n",
    "  'deprel': 'case',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 18,\n",
    "  'form': 'its',\n",
    "  'lemma': 'its',\n",
    "  'upos': 'PRON',\n",
    "  'xpos': 'PRP$',\n",
    "  'feats': {'Gender': 'Neut',\n",
    "   'Number': 'Sing',\n",
    "   'Person': '3',\n",
    "   'Poss': 'Yes',\n",
    "   'PronType': 'Prs'},\n",
    "  'head': 19,\n",
    "  'deprel': 'nmod:poss',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 19,\n",
    "  'form': 'relevance',\n",
    "  'lemma': 'relevance',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 16,\n",
    "  'deprel': 'obl',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}},\n",
    " {'id': 20,\n",
    "  'form': ',',\n",
    "  'lemma': ',',\n",
    "  'upos': 'PUNCT',\n",
    "  'xpos': ',',\n",
    "  'feats': None,\n",
    "  'head': 21,\n",
    "  'deprel': 'punct',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 21,\n",
    "  'form': 'interestingness',\n",
    "  'lemma': 'interestingness',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 19,\n",
    "  'deprel': 'conj',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}},\n",
    " {'id': 22,\n",
    "  'form': ',',\n",
    "  'lemma': ',',\n",
    "  'upos': 'PUNCT',\n",
    "  'xpos': ',',\n",
    "  'feats': None,\n",
    "  'head': 27,\n",
    "  'deprel': 'punct',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 23,\n",
    "  'form': 'and',\n",
    "  'lemma': 'and',\n",
    "  'upos': 'CCONJ',\n",
    "  'xpos': 'CC',\n",
    "  'feats': None,\n",
    "  'head': 27,\n",
    "  'deprel': 'cc',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 24,\n",
    "  'form': 'even',\n",
    "  'lemma': 'even',\n",
    "  'upos': 'ADV',\n",
    "  'xpos': 'RB',\n",
    "  'feats': None,\n",
    "  'head': 27,\n",
    "  'deprel': 'advmod',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 25,\n",
    "  'form': 'its',\n",
    "  'lemma': 'its',\n",
    "  'upos': 'PRON',\n",
    "  'xpos': 'PRP$',\n",
    "  'feats': {'Gender': 'Neut',\n",
    "   'Number': 'Sing',\n",
    "   'Person': '3',\n",
    "   'Poss': 'Yes',\n",
    "   'PronType': 'Prs'},\n",
    "  'head': 27,\n",
    "  'deprel': 'nmod:poss',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 26,\n",
    "  'form': 'aesthetic',\n",
    "  'lemma': 'aesthetic',\n",
    "  'upos': 'ADJ',\n",
    "  'xpos': 'JJ',\n",
    "  'feats': {'Degree': 'Pos'},\n",
    "  'head': 27,\n",
    "  'deprel': 'amod',\n",
    "  'deps': None,\n",
    "  'misc': None},\n",
    " {'id': 27,\n",
    "  'form': 'appeal',\n",
    "  'lemma': 'appeal',\n",
    "  'upos': 'NOUN',\n",
    "  'xpos': 'NN',\n",
    "  'feats': {'Number': 'Sing'},\n",
    "  'head': 19,\n",
    "  'deprel': 'conj',\n",
    "  'deps': None,\n",
    "  'misc': {'SpaceAfter': 'No'}},\n",
    " {'id': 28,\n",
    "  'form': '.',\n",
    "  'lemma': '.',\n",
    "  'upos': 'PUNCT',\n",
    "  'xpos': '.',\n",
    "  'feats': None,\n",
    "  'head': 16,\n",
    "  'deprel': 'punct',\n",
    "  'deps': None,\n",
    "  'misc': None}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d492bac8-0a69-41a0-adb1-2ef59ec67350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Thus ADV 16 advmod\n",
      "2 , PUNCT 1 punct\n",
      "3 the DET 4 det\n",
      "4 time NOUN 16 nsubj\n",
      "5 it PRON 6 nsubj\n",
      "6 takes VERB 4 acl:relcl\n",
      "7 and CCONJ 9 cc\n",
      "8 the DET 9 det\n",
      "9 ways NOUN 4 conj\n",
      "10 of SCONJ 12 mark\n",
      "11 visually ADV 12 advmod\n",
      "12 exploring VERB 9 acl\n",
      "13 an DET 14 det\n",
      "14 artwork NOUN 12 obj\n",
      "15 can AUX 16 aux\n",
      "16 inform VERB 0 root\n",
      "17 about ADP 19 case\n",
      "18 its PRON 19 nmod:poss\n",
      "19 relevance NOUN 16 obl\n",
      "20 , PUNCT 21 punct\n",
      "21 interestingness NOUN 19 conj\n",
      "22 , PUNCT 27 punct\n",
      "23 and CCONJ 27 cc\n",
      "24 even ADV 27 advmod\n",
      "25 its PRON 27 nmod:poss\n",
      "26 aesthetic ADJ 27 amod\n",
      "27 appeal NOUN 19 conj\n",
      "28 . PUNCT 16 punct\n"
     ]
    }
   ],
   "source": [
    "# now we can iterate through the AVMs for this sentence, and \n",
    "# print informati0n for each one\n",
    "for token in sentence10:\n",
    "    print(token[\"id\"], token[\"form\"], token[\"upos\"], \n",
    "          token[\"head\"], token[\"deprel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb7c79-8b4e-43e5-a09b-5e2baede941a",
   "metadata": {},
   "source": [
    "Now say we want to determine how often we have subject-verb-object (SVO) versus SOV versus VSO etc. in a Universal Dependencies corpus. To do that, we would like to have an AVM for a word that includes all its dependents. For the verb \"inform\" in the sentence above, we would like the AVM to list that \"time\" (word 4) is the nsubj of \"inform\", and \"relevance\" (word 19) is its obl:\n",
    "\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "\\text{form:} & inform\\\\\n",
    "\\text{id:} & 16\\\\\n",
    "\\text{upos:} & VERB\\\\\n",
    "\\text{dep:} & \\[ \\left[\\begin{array}{ll}\n",
    "\\text{id:} & 4\\\\\n",
    "\\text{deprel:} & nsubj\\end{array}\\right], \n",
    "\\left[\\begin{array}{ll}\n",
    "\\text{id:} & 19\\\\\n",
    "\\text{deprel:} & obl\\end{array}\\right]\\]\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "As a Python data structure, this AVM is rather complex: It is a dictionary, but under the key \"dep\" the value is a list of dictionaries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8811f49f-7ff4-4b54-9589-3b026a6fe6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inform_avm_with_deps = { \"form\" : \"inform\",\n",
    "                        \"id\" : 16,\n",
    "                        \"upos\" : \"VERB\",\n",
    "                        \"dep\" : [ {\"id\" : 4, \"deprel\" : \"nsubj\"}, \n",
    "                                  {\"id\" : 19, \"deprel\" : \"obl\"}]\n",
    "                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b79e50-3d38-4a8f-8e11-76f89e33d83c",
   "metadata": {},
   "source": [
    "Here is how we make a version of sentence 10 that has such an AVM for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "807e8456-009c-4331-a75c-2dc41bf90d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'Thus',\n",
       "  'id': 1,\n",
       "  'upos': 'ADV',\n",
       "  'dep': [{'id': 2, 'deprel': 'punct'}]},\n",
       " {'form': ',', 'id': 2, 'upos': 'PUNCT', 'dep': []},\n",
       " {'form': 'the', 'id': 3, 'upos': 'DET', 'dep': []},\n",
       " {'form': 'time',\n",
       "  'id': 4,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 3, 'deprel': 'det'},\n",
       "   {'id': 6, 'deprel': 'acl:relcl'},\n",
       "   {'id': 9, 'deprel': 'conj'}]},\n",
       " {'form': 'it', 'id': 5, 'upos': 'PRON', 'dep': []},\n",
       " {'form': 'takes',\n",
       "  'id': 6,\n",
       "  'upos': 'VERB',\n",
       "  'dep': [{'id': 5, 'deprel': 'nsubj'}]},\n",
       " {'form': 'and', 'id': 7, 'upos': 'CCONJ', 'dep': []},\n",
       " {'form': 'the', 'id': 8, 'upos': 'DET', 'dep': []},\n",
       " {'form': 'ways',\n",
       "  'id': 9,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 7, 'deprel': 'cc'},\n",
       "   {'id': 8, 'deprel': 'det'},\n",
       "   {'id': 12, 'deprel': 'acl'}]},\n",
       " {'form': 'of', 'id': 10, 'upos': 'SCONJ', 'dep': []},\n",
       " {'form': 'visually', 'id': 11, 'upos': 'ADV', 'dep': []},\n",
       " {'form': 'exploring',\n",
       "  'id': 12,\n",
       "  'upos': 'VERB',\n",
       "  'dep': [{'id': 10, 'deprel': 'mark'},\n",
       "   {'id': 11, 'deprel': 'advmod'},\n",
       "   {'id': 14, 'deprel': 'obj'}]},\n",
       " {'form': 'an', 'id': 13, 'upos': 'DET', 'dep': []},\n",
       " {'form': 'artwork',\n",
       "  'id': 14,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 13, 'deprel': 'det'}]},\n",
       " {'form': 'can', 'id': 15, 'upos': 'AUX', 'dep': []},\n",
       " {'form': 'inform',\n",
       "  'id': 16,\n",
       "  'upos': 'VERB',\n",
       "  'dep': [{'id': 1, 'deprel': 'advmod'},\n",
       "   {'id': 4, 'deprel': 'nsubj'},\n",
       "   {'id': 15, 'deprel': 'aux'},\n",
       "   {'id': 19, 'deprel': 'obl'},\n",
       "   {'id': 28, 'deprel': 'punct'}]},\n",
       " {'form': 'about', 'id': 17, 'upos': 'ADP', 'dep': []},\n",
       " {'form': 'its', 'id': 18, 'upos': 'PRON', 'dep': []},\n",
       " {'form': 'relevance',\n",
       "  'id': 19,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 17, 'deprel': 'case'},\n",
       "   {'id': 18, 'deprel': 'nmod:poss'},\n",
       "   {'id': 21, 'deprel': 'conj'},\n",
       "   {'id': 27, 'deprel': 'conj'}]},\n",
       " {'form': ',', 'id': 20, 'upos': 'PUNCT', 'dep': []},\n",
       " {'form': 'interestingness',\n",
       "  'id': 21,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 20, 'deprel': 'punct'}]},\n",
       " {'form': ',', 'id': 22, 'upos': 'PUNCT', 'dep': []},\n",
       " {'form': 'and', 'id': 23, 'upos': 'CCONJ', 'dep': []},\n",
       " {'form': 'even', 'id': 24, 'upos': 'ADV', 'dep': []},\n",
       " {'form': 'its', 'id': 25, 'upos': 'PRON', 'dep': []},\n",
       " {'form': 'aesthetic', 'id': 26, 'upos': 'ADJ', 'dep': []},\n",
       " {'form': 'appeal',\n",
       "  'id': 27,\n",
       "  'upos': 'NOUN',\n",
       "  'dep': [{'id': 22, 'deprel': 'punct'},\n",
       "   {'id': 23, 'deprel': 'cc'},\n",
       "   {'id': 24, 'deprel': 'advmod'},\n",
       "   {'id': 25, 'deprel': 'nmod:poss'},\n",
       "   {'id': 26, 'deprel': 'amod'}]},\n",
       " {'form': '.',\n",
       "  'id': 28,\n",
       "  'upos': 'PUNCT',\n",
       "  'dep': [{'id': 16, 'deprel': 'root'}]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reformat_sentence(sentence):\n",
    "    # first we initialize each AVM to have an empty dependencies list\n",
    "    sentence_reformatted = [ ]\n",
    "    for token in sentence:\n",
    "        sentence_reformatted.append( { \"form\" : token[\"form\"], \n",
    "                                    \"id\" : token[\"id\"],\n",
    "                                    \"upos\" : token[\"upos\"],\n",
    "                                    \"dep\" : [ ]\n",
    "                                  } )\n",
    "\n",
    "    # now we add dependencies\n",
    "    for token in sentence:\n",
    "        # looking up the head of this token. index is that head minus one.\n",
    "        myhead_ix = token[\"head\"] - 1\n",
    "        # print(token[\"form\"], token[\"id\"], token[\"head\"], sentence10_reformat[myhead_ix][\"form\"])\n",
    "        # adding this token to the head's dependencies\n",
    "        sentence_reformatted[ myhead_ix ][\"dep\"].append({ \"id\" : token[\"id\"],\n",
    "                                                       \"deprel\" : token[\"deprel\"]})\n",
    "\n",
    "    return sentence_reformatted\n",
    "    \n",
    "reformat_sentence(sentence10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07568beb-0ea6-45aa-872b-9edcf2122ae1",
   "metadata": {},
   "source": [
    "Based on this data structure, we can determine whether the subject is before the verb: If so, its ID is lower than that of the verb. We can also determine whether the subject is before the object: If so, its ID is lower than that of the the object.\n",
    "\n",
    "We can also see how far away from the verb the subject is, by computing the difference between the IDs of the verb and its subject. In the same way, we can determine how far away from the verb the direct object is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859b902d-2298-4c70-bf1c-d8d5e7f94821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b477748-872d-4a65-ab95-579e6a5a17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences[10])\n",
    "# print(sentences[10].metadata)\n",
    "def finderskeepers(sentences, sample_size):\n",
    "    fit_generalization = []\n",
    "    possible_exceptions = []\n",
    "    current = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word[\"upos\"] == \"PRON\" and word[\"deprel\"] == \"obj\":\n",
    "                if word[\"head\"] != None and sentence[word[\"head\"]-1][\"upos\"] == \"VERB\":\n",
    "                    current[\"sentence\"] = sentence.metadata['text']\n",
    "                    current[\"PRON\"] = word\n",
    "                    current[\"VERB\"] = sentence[word[\"head\"]-1]\n",
    "                    if word[\"id\"] != 0 and word[\"id\"]-1 == word[\"head\"]:\n",
    "                        if current not in fit_generalization:\n",
    "                            fit_generalization.append(current.copy())\n",
    "                    elif word[\"id\"]-1 != word[\"head\"]: # != gets all possible exceptions\n",
    "                        if current not in possible_exceptions:\n",
    "                            possible_exceptions.append(current.copy())\n",
    "    \n",
    "    print(f\"Sentences that fit the generalization: {len(fit_generalization)}\\n\")\n",
    "    \n",
    "    fitted_samples = random.sample(fit_generalization, sample_size)\n",
    "    \n",
    "    for entry in fitted_samples:\n",
    "        print(f'PRON: {entry[\"PRON\"]}, VERB: {entry[\"VERB\"]}\\n Sentence: {entry[\"sentence\"]}\\n')\n",
    "    \n",
    "    print(f\"\\nSentences that may (or may not) be exceptions: {len(possible_exceptions)}\\n\")\n",
    "    \n",
    "    fitted_samples = random.sample(possible_exceptions, sample_size)\n",
    "    \n",
    "    for entry in fitted_samples:\n",
    "        print(f'PRON: {entry[\"PRON\"]}, VERB: {entry[\"VERB\"]}\\n Sentence: {entry[\"sentence\"]}\\n')\n",
    "    \n",
    "    #print(count)\n",
    "    print(f\"\\nTotal sentences with Pronouns linked to Verbs: {len(fit_generalization)+len(possible_exceptions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa1ff487-b57f-4d55-952b-901a4d78b3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences that fit the generalization: 995\n",
      "\n",
      "PRON: it, VERB: like\n",
      " Sentence: And I really like it.\n",
      "\n",
      "PRON: her, VERB: distract\n",
      " Sentence: Back out in the mall, Cara is wailing, which could start an asthma attack, so to distract her I say, “You want a cookie?”\n",
      "\n",
      "PRON: em, VERB: get\n",
      " Sentence: You know then, they have to, like, keep em, away from anything, you know, get em really in the soft ground, and, no hard pebbles, or hard clods of dirt or anything?\n",
      "\n",
      "PRON: me, VERB: Let\n",
      " Sentence: Let me see if I can —\n",
      "\n",
      "PRON: someone, VERB: want\n",
      " Sentence: Because I want someone who's going to treat people nicely and well.\n",
      "\n",
      "\n",
      "Sentences that may (or may not) be exceptions: 156\n",
      "\n",
      "PRON: that, VERB: use\n",
      " Sentence: One of them is the channels that NBC as the broadcasting rights owner for the United States will use to air the Paralympic Games on.\n",
      "\n",
      "PRON: what, VERB: figure\n",
      " Sentence: No, I was just trying to figure out what we spent already.\n",
      "\n",
      "PRON: what, VERB: tell\n",
      " Sentence: And he said I tell you what I'll do, make you a deal.\n",
      "\n",
      "PRON: that, VERB: used\n",
      " Sentence: But I will point out before we get to that, that the EU are putting together regulation which will require people who create LLMs to declare the copyrighted material that they used in the training corpora, which I think is a really neat solution to this.\n",
      "\n",
      "PRON: What, VERB: say\n",
      " Sentence: What do you say at the end of the video?\n",
      "\n",
      "\n",
      "Total sentences with Pronouns linked to Verbs: 1151\n"
     ]
    }
   ],
   "source": [
    "finderskeepers(sentences, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bc77c5e-9ebd-4034-9b47-29f88c4fb267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newdoc id = 1\n",
      "# sent_id = sv_lines-ud-train-doc1-1\n",
      "# text = Visa alla\n",
      "# text_en = Show All\n",
      "1\tVisa\tvisa\tVERB\tIMP-ACT\tMood=Imp|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n",
      "2\talla\tall\tPRON\tTOT-PL-NOM\tDefinite=Ind|Number=Plur|PronType=Tot\t1\tobj\t_\t_\n",
      "\n",
      "# sent_id = sv_lines-ud-train-doc1-2\n",
      "# text = Om ANSI SQL-frågeläge\n",
      "# text_en = About ANSI SQL query mode\n",
      "1\tOm\tom\tADP\t_\t_\t3\tcase\t_\t_\n",
      "2\tANSI\tANSI\tPROPN\tSG-NOM\tCase=Nom\t3\tnmod\t_\t_\n",
      "3\tSQL-frågeläge\tSQLfrågeläge\tNOUN\tIND-NOM\tCase=Nom|Definite=Ind|Gender=Neut|Number=Sing\t0\troot\t_\t_\n",
      "\n",
      "# sent_id = sv_lines-ud-train-doc1-3\n",
      "# text = En del av innehållet i det här avsnittet kanske inte gäller för vissa språk.\n",
      "# text_en = Some of the content in this topic may not be applicable to some languages.\n",
      "1\tEn\ten\tDET\tSG-IND\tDefinite=Ind|Gender=Com|Number=Sing|PronType=Art\t2\tdet\t_\t_\n",
      "2\tdel\tdel\tNOUN\tSG-IND-NOM\tCase=Nom|Definite=Ind|Gender=Com|Number=Sing\t11\tnsubj\t_\t_\n",
      "3\tav\tav\tADP\t_\t_\t4\tcase\t_\t_\n",
      "4\tinnehållet\tinnehåll\tNOUN\tSG-DEF-NOM\tCase=Nom|Definite=Def|Gender=Neut|Number=Sing\t2\t\n"
     ]
    }
   ],
   "source": [
    "# AND NOW WE BEGIN TO EXAMINE THE SWEDISH DATA\n",
    "with open(\"sv_lines-ud-train.conllu\", encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "sentences_swedish = conllu.parse(data)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6849a54-8687-438f-a154-4eb224f4a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swedishchecker(sentences, sample_size):\n",
    "    fit_generalization = []\n",
    "    possible_exceptions = []\n",
    "    current = {}\n",
    "    exceptions = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word[\"xpos\"] == \"NEG\":\n",
    "                if word[\"head\"] != None and sentence[word[\"head\"]-1][\"upos\"] == \"VERB\":\n",
    "                    current[\"sentence\"] = sentence.metadata['text']\n",
    "                    try:\n",
    "                        current[\"sentence-E\"] = sentence.metadata['text_en']\n",
    "                    except:\n",
    "                        try:\n",
    "                            current[\"sentence-E\"] = sentence.metadata['Text_en']\n",
    "                        except:\n",
    "                            print(f\"Error for sentence: {sentence.metadata}\")\n",
    "                            current[\"sentence-E\"] = None\n",
    "                    current[\"NEG\"] = word\n",
    "                    current[\"VERB\"] = sentence[word[\"head\"]-1]\n",
    "                    if word[\"id\"] != 0 and word[\"id\"]-1 >= word[\"head\"]:\n",
    "                        if current not in fit_generalization:\n",
    "                            fit_generalization.append(current.copy())\n",
    "                    elif word[\"id\"]-1 != word[\"head\"]:\n",
    "                        if current not in possible_exceptions:\n",
    "                            possible_exceptions.append(current.copy())\n",
    "                            exceptions.append(sentence)\n",
    "    \n",
    "    print(f\"Sentences that fit the generalization: {len(fit_generalization)}\\n\")\n",
    "    \n",
    "    fitted_samples = random.sample(fit_generalization, min(sample_size,len(fit_generalization)))\n",
    "    \n",
    "    for entry in fitted_samples:\n",
    "        print(f'NEG: {entry[\"NEG\"]}, VERB: {entry[\"VERB\"]}\\n Sentence: {entry[\"sentence\"]}\\n English Translation: {entry[\"sentence-E\"]}\\n')\n",
    "    \n",
    "    print(f\"\\nSentences that may (or may not) be exceptions: {len(possible_exceptions)}\\n\")\n",
    "    \n",
    "    fitted_samples = random.sample(possible_exceptions, min(sample_size,len(possible_exceptions)))\n",
    "    \n",
    "    for entry in fitted_samples:\n",
    "        print(f'NEG: {entry[\"NEG\"]}, VERB: {entry[\"VERB\"]}\\n Sentence: {entry[\"sentence\"]}\\n English Translation: {entry[\"sentence-E\"]}\\n')\n",
    "    \n",
    "    print(f\"\\nTotal sentences with Negations linked to Verbs: {len(fit_generalization)+len(possible_exceptions)}\")\n",
    "    return exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "545fa601-563f-4b4a-b224-54d1620a870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences that fit the generalization: 164\n",
      "\n",
      "NEG: aldrig, VERB: lyssnade\n",
      " Sentence: Mor lyssnade aldrig på vad farmor läste, men hon kände att hon gjorde sin plikt inför släkten och Gud och det besparade henne besväret att gå i kyrkan.\n",
      " English Translation: My mother never listened to what my grandmother read, but she felt she was doing her duty by her family and by God, and it saved her the trouble of going to church.\n",
      "\n",
      "NEG: inte, VERB: fanns\n",
      " Sentence: Den här mannens göra var att slå tegel – det hade åtminstone jag fått höra; men det fanns inte så mycket som en tegelskärva på hela stationen, och han hade gått där ett helt år och bara väntat.\n",
      " English Translation: The business intrusted to this fellow was the making of bricks – so I had been informed; but there wasn't a fragment of a brick anywhere in the station, and he had been there more than a year – waiting.\n",
      "\n",
      "NEG: inte, VERB: gjorde\n",
      " Sentence: Förresten gjorde de inte annat än väntade allesammans – alla de sexton, tjugu pilgrimerna – på någonting; och att döma av deras uppsyn var det ingen motbjudande sysselsättning, fast det enda den inbringade dem, vad jag kunde se, var ohälsa.\n",
      " English Translation: However, they were all waiting – all the sixteen or twenty pilgrims of them – for something; and upon my word it did not seem an uncongenial occupation, from the way they took it, though the only thing that ever came to them was disease – as far as I could see.\n",
      "\n",
      "NEG: inte, VERB: konverteras\n",
      " Sentence: Underformulär och underrapporter i ett formulär eller en rapport konverteras inte när du använder kommandot Spara som.\n",
      " English Translation: Subforms and subreports on a form or report are not converted when you carry out the Save As command.\n",
      "\n",
      "NEG: inte, VERB: stämmer\n",
      " Sentence: Ni gav ut en bok för flera år sen, stämmer inte det?\n",
      " English Translation: You did a book several years ago, didn't you?\n",
      "\n",
      "NEG: inte, VERB: gick\n",
      " Sentence: Stillman talade aldrig med någon, gick inte in i några affärer, log aldrig.\n",
      " English Translation: Stillman did not talk to anyone, did not go into any stores, did not smile.\n",
      "\n",
      "\n",
      "Sentences that may (or may not) be exceptions: 303\n",
      "\n",
      "NEG: inte, VERB: besvärades\n",
      " Sentence: Den tjugosexåriga Vivien Bayley – drottninglik med sitt sköna, behärskade, disciplinerade ansikte – kom då och då över till Bray mellan ansvarskännande rekognosceringsturer i tältet för att övertyga sig om att den eller den unga flickan inte besvärades allt för mycket av en eller annan äldre eller mera påtagligt berusad individ, eller att den eller den unge mannen inte ignorerades av flickorna, som borde ägna honom större uppmärksamhet.\n",
      " English Translation: Vivien Bayley, queenly at twenty-six, with her beautiful, well-mannered, disciplined face, came to hover beside Bray between responsible permutations about the room to make sure that this young girl was not being bothered too much by the attentions of someone older and rather drunk, or that young man was not being overlooked by the girls who ought to be taking notice of him.\n",
      "\n",
      "NEG: inte, VERB: gjorde\n",
      " Sentence: Jag delade förmodligen inte hans inställning-- han hoppades verkligen att jag inte gjorde det-- men han var inte längre intresserad av ytligt umgänge:\n",
      " English Translation: I probably didn't share his feelings– he hoped, really, that I did n't– but he was no longer interested in socialising;\n",
      "\n",
      "NEG: aldrig, VERB: behöva\n",
      " Sentence: Dessa förträffliga personer som har stuckit söder ut, till Rhodesia och Sydafrika, där de kan vara säkra på att aldrig behöva ha en svart man i rätten som avkunnar lika partiska domar som en vit.\n",
      " English Translation: Those worthy fellows who've gone down South to Rhodesia and South Africa where they can feel confident they'll never have a black man on the Bench to give a verdict as biased as a white man's.\n",
      "\n",
      "NEG: inte, VERB: tillåta\n",
      " Sentence: För det andra: Vi bör inte tillåta någon förskjutning ad calendas graecas och inte införa någon inskränkning till fysiska personer.\n",
      " English Translation: Secondly, we should not allow this to be postponed indefinitely, and we should not introduce provisions restricted to physical persons.\n",
      "\n",
      "NEG: inte, VERB: gå\n",
      " Sentence: När fartyget passerade Stromboli nattetid flöt det en strimma av karmosinröd lava från vulkanen men sjömännen ville inte gå ifrån TV-apparaten för att titta på detta naturfenomen.\n",
      " English Translation: When the ship passed Stromboli at night, there was a streak of crimson lava flowing from the volcano and the sailors wouldn't leave the television set to look at this natural phenomenon.\n",
      "\n",
      "NEG: inte, VERB: gäller\n",
      " Sentence: En del av innehållet i det här avsnittet kanske inte gäller för vissa språk.\n",
      " English Translation: Some of the content in this topic may not be applicable to some languages.\n",
      "\n",
      "\n",
      "Total sentences with Negations linked to Verbs: 467\n"
     ]
    }
   ],
   "source": [
    "exceptions = swedishchecker(sentences_swedish, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c37fd1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"'metadata'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m tok.keys():\n\u001b[32m      6\u001b[39m         strung += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtok[attr]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m    \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     strung += \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtok\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrung\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sokkm\\Documents\\SCHOOL\\2025-2026\\ling409\\409-swedish-final\\.venv\\Lib\\site-packages\\conllu\\models.py:40\u001b[39m, in \u001b[36mToken.__missing__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.MAPPING:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get(\u001b[38;5;28mself\u001b[39m.MAPPING[key])\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m + key + \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"'metadata'\""
     ]
    }
   ],
   "source": [
    "# print(exceptions)\n",
    "for tokList in exceptions:\n",
    "    strung = \"\"\n",
    "    for tok in tokList:\n",
    "        for attr in tok.keys():\n",
    "            strung += f\"{tok[attr]}    \"\n",
    "        strung += f'{tok[\"metadata\"]}\\n'\n",
    "    print(f\"{strung}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "142aaa49-c523-4dcb-ac5e-95851b1e2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 EXTRACTING SEMANTIC GENERALIZATIONS ABOUT VERBS\n",
    "# extract all the verbs in corpus and sort them by frequency\n",
    "def verb_frequencies(sentences):\n",
    "    verb_freq = {}\n",
    "    # verbs = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word[\"upos\"] == \"VERB\":\n",
    "                verb = word[\"lemma\"]\n",
    "                verb_freq[verb] = verb_freq.get(verb,0) + 1\n",
    "    verbs = list(verb_freq.keys())\n",
    "    print(f\"There are {len(verbs)} verbs used in the corpus.\")\n",
    "    #print(f\"Those verbs are: {verbs}\")\n",
    "    sorted_by_frequency_desc = sorted(verb_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "    first_five = sorted_by_frequency_desc[:5] # optionally random sample the top 20%\n",
    "    # it would be better if we can print this in a nicer format, and include the english translation\n",
    "    print(f\"The highest frequency verbs are: {first_five}\")\n",
    "    # then find middle five using some multiplication for the sorted list?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4752535e-5685-4e6b-99e1-50e0a906b3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1308 verbs used in the corpus.\n",
      "The highest frequency verbs are: [('säga', 294), ('ha', 225), ('komma', 188), ('se', 185), ('gå', 180)]\n"
     ]
    }
   ],
   "source": [
    "verb_frequencies(sentences_swedish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8536a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sets(sentences, verb):\n",
    "    sets = {\"verb\": verb, \"subjects\": set(), \"objects\": set(), \"modifiers\": set(), \"before\": set(), \"after\": set()}\n",
    "    for sentence in sentences:\n",
    "        words = [x['lemma'] for x in sentence]\n",
    "        if (verb in words):\n",
    "            word_id = words.index(verb)+1\n",
    "            sets[\"before\"].add(words[word_id-2])\n",
    "            sets[\"after\"].add(words[word_id])\n",
    "            for word in sentence:\n",
    "                if(word[\"deprel\"] in [\"obj\", \"nsubj\", \"iobj\", \"advmod\"] and word[\"head\"] == word_id):\n",
    "                    match word[\"deprel\"]:\n",
    "                        case \"obj\" | \"iobj\":\n",
    "                            sets[\"objects\"].add(word[\"lemma\"])\n",
    "                        case \"nsubj\":\n",
    "                            sets[\"subjects\"].add(word[\"lemma\"])\n",
    "                        case \"advmod\":\n",
    "                            sets[\"modifiers\"].add(word[\"lemma\"])\n",
    "                    \n",
    "    return sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60cd3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word2vec model\n",
    "def make_W2V(conllu_corpus):\n",
    "    sentences = []\n",
    "    for tokList in conllu_corpus:\n",
    "        sent = []\n",
    "        for token in tokList:\n",
    "            if token != \"metadata\":\n",
    "                sent.append(token[\"lemma\"])\n",
    "        sentences.append(sent)\n",
    "        \n",
    "    space = Word2Vec(sentences, epochs=10, min_count=10, vector_size=300, sg = 1)\n",
    "    return space.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71411e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest(k, space, centroid):\n",
    "    return space.most_similar(centroid)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffceb7-4ab2-46d3-a2fc-3c184675d792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lyssna', 0.9959690570831299),\n",
       " ('folk', 0.9938782453536987),\n",
       " ('förrän', 0.9936103820800781),\n",
       " ('först', 0.9927297234535217),\n",
       " ('någonstans', 0.9926219582557678)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_centroid(set: set, space):\n",
    "    total = []\n",
    "    for token in set:\n",
    "        total.append(space[token])\n",
    "    \n",
    "    sum = reduce(lambda x, y: x + y, total) / len(set)\n",
    "    return space.similar_by_vector(sum)[0]\n",
    "\n",
    "sets = gen_sets(sentences_swedish, \"heta\")\n",
    "space = make_W2V(sentences_swedish)\n",
    "centroid = find_centroid(sets[\"subjects\"], space)\n",
    "k_nearest(5, space, centroid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad31bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
